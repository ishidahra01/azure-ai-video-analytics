{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Video Analytics Demo\n",
    "\n",
    "## Preprocessing video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-search-documents==11.6.0b4\n",
    "! pip install openai python-dotenv azure-identity cohere azure-ai-vision-imageanalysis\n",
    "! pip install azure-storage-blob\n",
    "! pip install opencv-python pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    AIServicesVisionParameters,\n",
    "    AIServicesVisionVectorizer,\n",
    "    AIStudioModelCatalogName,\n",
    "    AzureMachineLearningVectorizer,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIModelName,\n",
    "    AzureOpenAIParameters,\n",
    "    BlobIndexerDataToExtract,\n",
    "    BlobIndexerParsingMode,\n",
    "    CognitiveServicesAccountKey,\n",
    "    DefaultCognitiveServicesAccount,\n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    FieldMapping,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    IndexerExecutionStatus,\n",
    "    IndexingParameters,\n",
    "    IndexingParametersConfiguration,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    ScalarQuantizationCompressionConfiguration,\n",
    "    ScalarQuantizationParameters,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SearchIndexer,\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataIdentity,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SearchIndexerSkillset,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticSearch,\n",
    "    SimpleField,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile,\n",
    "    VisionVectorizeSkill\n",
    ")\n",
    "from azure.search.documents.models import (\n",
    "    HybridCountAndFacetMode,\n",
    "    HybridSearch,\n",
    "    SearchScoreThreshold,\n",
    "    VectorizableTextQuery,\n",
    "    VectorizableImageBinaryQuery,\n",
    "    VectorizableImageUrlQuery,\n",
    "    VectorSimilarityThreshold,\n",
    ")\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display, HTML\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "AZURE_AI_VISION_API_KEY = os.getenv(\"AZURE_AI_VISION_API_KEY\")\n",
    "AZURE_AI_VISION_ENDPOINT = os.getenv(\"AZURE_AI_VISION_ENDPOINT\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "BLOB_CONNECTION_STRING = os.getenv(\"BLOB_CONNECTION_STRING\")\n",
    "BLOB_CONTAINER_NAME = os.getenv(\"BLOB_CONTAINER_NAME\")\n",
    "INDEX_NAME = \"azure-ai-video-analytics\"\n",
    "AZURE_SEARCH_ADMIN_KEY = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-specified parameter\n",
    "USE_AAD_FOR_SEARCH = False  # Set this to False to use API key for authentication\n",
    "\n",
    "def authenticate_azure_search(api_key=None, use_aad_for_search=False):\n",
    "    if use_aad_for_search:\n",
    "        print(\"Using AAD for authentication.\")\n",
    "        credential = DefaultAzureCredential()\n",
    "    else:\n",
    "        print(\"Using API keys for authentication.\")\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"API key must be provided if not using AAD for authentication.\")\n",
    "        credential = AzureKeyCredential(api_key)\n",
    "    return credential\n",
    "\n",
    "azure_search_credential = authenticate_azure_search(api_key=AZURE_SEARCH_ADMIN_KEY, use_aad_for_search=USE_AAD_FOR_SEARCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting frames from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, generate_blob_sas, BlobSasPermissions\n",
    "import datetime\n",
    "\n",
    "# Setting for Azure Blob Storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(BLOB_CONNECTION_STRING)\n",
    "\n",
    "def upload_to_blob(file_path, blob_name):\n",
    "    blob_client = blob_service_client.get_blob_client(container=BLOB_CONTAINER_NAME, blob=blob_name)\n",
    "    with open(file_path, \"rb\") as data:\n",
    "        blob_client.upload_blob(data, overwrite=True)\n",
    "    \n",
    "    start_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "    expiry_time = start_time + datetime.timedelta(days=1)\n",
    "    \n",
    "    sas_token = generate_blob_sas(\n",
    "        account_name=blob_client.account_name,\n",
    "        container_name=blob_client.container_name,\n",
    "        blob_name=blob_client.blob_name,\n",
    "        account_key=blob_service_client.credential.account_key,\n",
    "        permission=BlobSasPermissions(read=True),\n",
    "        expiry=expiry_time,\n",
    "        start=start_time\n",
    "    )\n",
    "    blob_url = f\"https://{blob_client.account_name}.blob.core.windows.net/{blob_client.container_name}/{blob_client.blob_name}\"\n",
    "    blob_url_with_sas = f\"{blob_url}?{sas_token}\"\n",
    "    return blob_url, blob_url_with_sas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: The openai-python library support for Azure OpenAI is in preview.\n",
    "#Note: This code sample requires OpenAI Python library version 1.0.0 or higher.\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "\n",
    "\n",
    "def create_caption_by_gpt(image_url):\n",
    "\tprint(\"Image URL:\", image_url)\n",
    "\tprint(\"Generating caption using GPT-4o model...\")\n",
    "\tclient = AzureOpenAI(\n",
    "\t\tazure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "\t\tapi_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "\t\tapi_version=\"2024-02-01\"\n",
    "\t)\n",
    "\n",
    "\tsystem_message = \"\"\"\n",
    "\tYou are an excellent caption creator. You must create a detailed caption for the image below with a maximum of 100 characters.\n",
    "\tThe created caption will be used to describe the image in the search index.\n",
    "\t\"\"\"\n",
    "\n",
    "\tmessage_text = [\n",
    "\t\t{\"role\":\"system\",\"content\":system_message},\n",
    "\t\t{\"role\":\"user\",\"content\": [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": \"text\",\n",
    "\t\t\t\t\"content\": f\"Please provide a caption for the image.\"\n",
    "\t\t\t},\n",
    "\t\t\t{ \n",
    "\t\t\t\t\"type\": \"image_url\",\n",
    "\t\t\t\t\"image_url\": {\n",
    "\t\t\t\t\t\"url\": image_url\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t]}\n",
    "\t]\n",
    "\n",
    "\n",
    "\tcompletion = client.chat.completions.create(\n",
    "\t\tmodel=\"gpt-4o\", # model = \"deployment_name\"\n",
    "\t\tmessages = message_text,\n",
    "\t\ttemperature=0,\n",
    "\t\tmax_tokens=200,\n",
    "\t\ttop_p=0.95,\n",
    "\t\tfrequency_penalty=0,\n",
    "\t\tpresence_penalty=0,\n",
    "\t\tstop=None\n",
    "\t\t)\n",
    "\tprint(completion.choices[0].message.content)\n",
    "\treturn completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, output_dir, interval=5):\n",
    "    # Get the video file name\n",
    "    video_filename = os.path.basename(video_path)\n",
    "    \n",
    "    # Create the output directory\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Load the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = frame_count / fps\n",
    "    \n",
    "    # List to save metadata\n",
    "    metadata = []\n",
    "\n",
    "    # Extract frames\n",
    "    frame_number = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        current_time = frame_number / fps\n",
    "        if current_time % interval < 1.0 / fps:\n",
    "            # Save the frame as an image\n",
    "            frame_filename = f\"{video_filename}_frame_{frame_number}.jpg\"\n",
    "            frame_filepath = os.path.join(output_dir, frame_filename)\n",
    "            cv2.imwrite(frame_filepath, frame)\n",
    "            \n",
    "            # Upload the frame image to Blob storage\n",
    "            blob_url, blob_url_with_sas = upload_to_blob(frame_filepath, frame_filename)\n",
    "            \n",
    "            # Generate a caption for the frame image\n",
    "            caption = create_caption_by_gpt(blob_url_with_sas)\n",
    "            \n",
    "            # Save the metadata\n",
    "            metadata.append({\n",
    "                'video_filename': video_filename,\n",
    "                'frame_filename': frame_filename,\n",
    "                'frame_url': blob_url,\n",
    "                'frame_number': frame_number,\n",
    "                'timestamp': current_time,\n",
    "                'caption': caption\n",
    "            })\n",
    "        \n",
    "        frame_number += 1\n",
    "\n",
    "    # Save metadata as a CSV\n",
    "    metadata_json_path = os.path.join(output_dir, f\"{video_filename}_metadata.json\")\n",
    "    with open(metadata_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    upload_to_blob(metadata_json_path, f\"{BLOB_CONTAINER_NAME}-metadata\")\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    print(f\"Frames and metadata have been saved in {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the video file\n",
    "video_path = 'data/keynote_ms_build_2024.mp4'\n",
    "output_dir = 'output_frames'\n",
    "extract_frames(video_path, output_dir, interval=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
